# CS231n_6ê°•

## Training Neural Networks Part I - Activation Function

2021.05.16

## 1. Activation Function

![./img/Untitled.png](./img/Untitled.png)

activation funtion, í™œì„±í™” í•¨ìˆ˜ëŠ” ë‹¤ìŒ ë‰´ëŸ°ì—ê²Œ ê°’ì„ ì „ë‹¬í• ì§€ ë§ì§€ë¥¼ ì •í•˜ëŠ”, ì¦‰ **ë‰´ëŸ°ì˜ í™œì„±í™” ì—¬ë¶€**ë¥¼ ì •í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ì´ë ‡ê²Œ í™œì„±í™” í•¨ìˆ˜ê°€ í•„ìš”í•œ ì´ìœ ëŠ”, í™œì„±í™”í•¨ìˆ˜ê°€ Wx+bë¡œ ì´ë£¨ì–´ì§„ linear functionì— **non-linearity**ë¥¼ ë”í•´ì£¼ê¸° ë•Œë¬¸ì´ë‹¤. (XOR ê°™ì´ linear functionìœ¼ë¡œëŠ” ì ˆëŒ€ í•´ê²°í• ìˆ˜ ì—†ëŠ” ë¬¸ì œë¥¼ í•´ê²°í• ìˆ˜ ìˆëŠ” ë°©ë²•ì´ activation functionì˜ ì‚¬ìš©ì´ë‹¤.)

activation functionì—ëŠ” ì—¬ëŸ¬ê°€ì§€ ì¢…ë¥˜ê°€ ìˆê³ , ì¢…ë¥˜ì— ë”°ë¼ì„œ ì–´ë–¤ ê¸°ì¤€ì¹˜ë¡œ í™œì„±í™” ì‹œí‚¬ì§€ê°€ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— í•™ìŠµ ê²°ê³¼ë„ ë‹¬ë¼ì§„ë‹¤.

![./img/_2021-05-16__11.20.00.png](./img/_2021-05-16__11.20.00.png)

### < Sigmoid >

![./img/_2021-05-17__3.33.43.png](./img/_2021-05-17__3.33.43.png)

$$\sigma(x) = \frac{1}{1+e^{â€‹â€‘x}}$$

ì „í†µì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ë˜ í•¨ìˆ˜. ê·¸ë˜í”„ë¡œ ì•Œ ìˆ˜ ìˆë“¯ì´ 0 ê³¼ 1 ì‚¬ì´ì˜ ì¶œë ¥ê°’ì„ ê°–ëŠ”ë‹¤ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. 

Sigmoidë¥¼ ì˜ˆì „ì—ëŠ” ë§ì´ ì‚¬ìš©í–ˆìœ¼ë‚˜ í˜„ì¬ëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ìœ ëŠ” ì„¸ê°€ì§€ í° ê²°í•¨ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

1. Saturated neurons **â€œkillâ€ the gradients**

$x$ê°€ -â™¾ï¸ë˜ëŠ” â™¾ï¸ë¡œ ë°œì‚°í•˜ëŠ” ê²½ìš°ì—ëŠ” ê°ê°ì˜ ê¸°ìš¸ê¸°ê°€ 0ìœ¼ë¡œ **saturated** í•œë‹¤. ì´ëŠ” Back Propagationì„ í• ë•Œ 0ì´ ê³±í•´ì§€ëŠ” ê²°ê³¼ë¥¼ ë‚³ê³  í•´ë‹¹ ë‰´ë ¨ì˜ ë¼ì¸ì„ ë¹„í™œì„±í™” ì‹œí‚¤ëŠ” ê²°ê³¼ê°€ ë°œìƒí•œë‹¤ â†’ **Vanising Gradient Problem**

2. Sigmoid outputs are **not zero-centered**

![./img/Untitled%201.png](./img/Untitled%201.png)

ë‰´ëŸ°ì˜ inputê°’(ìœ„ ì˜ˆì‹œì—ì„œëŠ” x)ê°€ í•­ìƒ ì–‘ìˆ˜ì˜ ê°’ì„ ê°–ëŠ”ë‹¤ë©´?

ê²°ë¡ ë¶€í„° ë§í•˜ìë©´ Wì— ëŒ€í•œ gradientê°€ í•­ìƒ ì–‘ìˆ˜ê±°ë‚˜ í•­ìƒ ìŒìˆ˜ë¡œ ê°™ì€ ë°©í–¥ì„ ê°–ëŠ”ë‹¤.

![./img/Untitled%202.png](./img/Untitled%202.png)

![./img/___(2)-2.jpg](./img/___(2)-2.jpg)

í•´ê²°ë°©ë²•ì€ **zero-mean data**

inputê°’ Xê°€ ì–‘ìˆ˜/ìŒìˆ˜ë¥¼ ëª¨ë‘ ê°€ì§€ê³  ìˆìœ¼ë©´ gradient wê°€ ì „ë¶€Positive / Negativeë¡œ ì›€ì§ì´ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤.

## < tanh >

![./img/_2021-05-17__4.24.04.png](./img/_2021-05-17__4.24.04.png)

- ì¶œë ¥ê°’ [-1,1]
- **zero centered** (nice)
- ê·¸ì¹˜ë§Œ **killing gradient** ë¬¸ì œ ì—¬ì „íˆ ë°œìƒ(ê¸°ìš¸ê¸°ê°€ flatí•œ ë¶€ë¶„ì—ì„œ)

## < ReLU >

![./img/_2021-05-17__4.26.19.png](./img/_2021-05-17__4.26.19.png)

$$f(x) = max(0,x)$$

ReLUí•¨ìˆ˜ë„ êµ‰ì¥íˆ ë§ì´ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ì¤‘ í•˜ë‚˜ë‹¤.

Sigmoidê°€ -â™¾ï¸ë˜ëŠ” â™¾ï¸ë¡œ ê°ˆìˆ˜ë¡ gradientê°€ 0ì´ ë˜ë©° vanishingí•œë‹¤ë©´, RELUëŠ” 0ì´í•˜ì˜ ê°’ì—ì„œ gradientê°€ 0ì´ëœë‹¤.

ì¥ì ìœ¼ë¡œëŠ”

- ê³„ì‚° íš¨ìœ¨ì´ ë›°ì–´ë‚˜ë‹¤, ê·¸ëƒ¥ maxê°’ ì°¾ëŠ” ê°„ë‹¨í•œ ì‹ì´ê¸°ì— expê°™ì€ ê³„ì‚°ê³¼ì •ì´ ë“¤ì–´ìˆëŠ” sigmoidë‚˜ tnahë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ë‹¤.
- ìƒë¬¼í•™ì  íƒ€ë‹¹ì„±ì´ ê°€ì¥ ë†’ë‹¤(?)

ë‹¨ì ìœ¼ë¡œëŠ”

- non- zero-centered
- ìŒìˆ˜ì—ì„œ saturated, vanishing gradient
- **DEAD ReLU**í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ì•„ë˜ì—ì„œ ì‚´í´ë³´ì

**DEAD ReLU** 

![./img/_2021-05-17__4.55.13.png](./img/_2021-05-17__4.55.13.png)

data cloud : all of our training data, ì´ ê·¸ë¦¼ì€ 2D weightì˜ ì˜ˆì‹œ

ReLUì˜ ì™¼ìª½ì— ëª¨ë“  ë°ì´í„°ê°€ ìœ„ì¹˜í•˜ê²Œ ë˜ëŠ” ìƒí™©..

ëª¨ë“  gradientê°€ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ì–´ë–¤ ì—…ë°ì´íŠ¸ë„ ë°œìƒí•˜ì§€ ì•Šê³  ê²°êµ­ ì£½ì–´ë²„ë¦¬ê²Œ ë˜ëŠ” ê²ƒ, ì´ˆê¸°í™”ë¥¼ ì˜ ì˜ëª»í•´ë²„ë¦¬ê±°ë‚˜ learning rateê°€ ë„ˆë¬´ í´ë•Œ ì´ëŸ°ë¬¸ì œê°€ ìƒê¸´ë‹¤.

Unfortunately, ReLU units can be fragile during training and can â€œdieâ€. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be â€œdeadâ€ (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue. 

## < Leaky ReLU >

![./img/_2021-05-17__5.11.49.png](./img/_2021-05-17__5.11.49.png)

$$f(x) = max(0.01x,x)$$

ReLUì˜ ë³€í˜•. â€œdying ReLUâ€ problemì„ ê³ ì¹˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡Œë‹¤.

## < PReLU >

$$f(x) = max(\alpha x,x)$$

leaky ReLUê°€ negative slopeì—ì„œ 0.01ì„ ê³±í•´ì„œ dyingì„ ë°©ì§€í–ˆë‹¤ë©´, PReLUëŠ” alphaë¥¼ ê³±í•´ì„œ **hyperparameterí™”** ì‹œí‚¨ë‹¤. 

ì•ŒíŒŒê°’ì„ hard-codeí•˜ì§€ ì•Šê³  backpropí•´ì„œ ìµœì ì˜ ì•ŒíŒŒê°’ì„ ì°¾ëŠ”ë‹¤.

### < ELU >

![./img/_2021-05-17__5.17.27.png](./img/_2021-05-17__5.17.27.png)

$$f(x) = \begin{cases}x & \text{if } x\gt 0\\ \alpha(exp(x)-1)& \text{if } x\le 0
\end{cases}$$

ì–´ì©Œêµ¬ì €ì©Œêµ¬~LUê°€ ì •ë§ ë§ë‹¤.. ELUëŠ” Exponential Linear Unitsì˜ ì•½ìì¸ë°,

- ReLUì˜ ì¥ì ì€ ë‹¤ ê°€ì¡Œë‹¤
- Closer to zero mean outputs
- í•˜ì§€ë§Œ negaitve regimeì—ì„œ "ê¸°ìš¸ê¸°"ê°€ ìˆëŠ” ëŒ€ì‹  0ìœ¼ë¡œ **saturated** ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤ëŠ” ì  â†’ í•˜ì§€ë§Œ ì˜¤íˆë ¤ saturationë•ë¶„ì— noiseì— robustí•˜ë‹¤ê³  ELU ë…¼ë¬¸ì—ì„œ ì£¼ì¥í•œë‹µë‹ˆë‹¤. ê´€ì‹¬ìˆìœ¼ì‹ ë¶„ì€ ğŸ‘‰ğŸ‘‰  [ELUë…¼ë¬¸](https://arxiv.org/pdf/1511.07289.pdf)

## < Maxout "Neuron" >

$$max(w_1^Tx + b_1, w_2^Tx + b_2)$$

Maxoutì€ ì—¬íƒœê» ë´ì™”ë˜ í™œì„±í•¨ìˆ˜ì™€ ì¡°ê¸ˆ ë‹¤ë¥¸ ëŠë‚Œì¸ë°, ReLU ì™€ Leaky ReLUë¥¼ í•©ì³ì„œ ë§Œë“¤ì—ˆë‹¤. ê°ê°ì˜ ë‹¨ì ì„ ìƒì‡„ì‹œí‚¤ì§€ë§Œ. ë‘ê°œì˜ ì„ í˜•í•¨ìˆ˜ë¥¼ ì·¨í•´ì„œ ë‰´ëŸ°ë‹¹ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ë‘ ë°°ê°€ ëœë‹¤ëŠ” ì 

---

ì§€ê¸ˆê¹Œì§€ activation functionì˜ ì¢…ë¥˜ì™€ ê°ê°ì˜ íŠ¹ì§•ë“¤ì— ëŒ€í•´ ì•Œì•„ë´¤ëŠ”ë°.

ì‹¤ì œë¡œëŠ” **ReLU**ê°€ ê°€ì¥ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ë‹¤ë§Œ, DEAD ReLUë¥¼ í”¼í•˜ê¸° ìœ„í•´ learning rateì— ì£¼ì˜í•´ì•¼ í•œë‹¤ëŠ” ì .

ê·¸ë¦¬ê³  Sigmoid í•¨ìˆ˜ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•˜ë„¤ìš”. ë²„ë ¤ì§„ ì‹œê·¸ëª¨ì´ë“œ ğŸ˜‡