\*본 글은 cs231n 14강의 내용을 정리한 글입니다.

오늘은 딥러닝의 강화학습에 대해 정리해보겠습니다.

'인공지능을 세계에, 특히 우리나라에 널리 알린 사건' 하면 떠오르는게 이세돌과 알파고의 대결입니다.

인간과 인공지능의 대결로 많은 관심을 받았었는데요, 사실 알파고는 오늘 다룰 강화학습 알고리즘을 사용합니다. 

대결로부터 5년이 지난 지금, 강화학습은 여전히 활발히 연구되고 있고, 스타크래프트도 강화학습으로 플레이 할 수 있을정도로 많은 발전이 있었습니다. 그렇다면 이 강화학습은 어떤 메커니즘으로 학습하고 작동하는걸까요?

### Reinforcement Learning

강화학습을 잘이해하기 위해서는 우선적으로 알아야하는 개념이 몇개 있습니다.

[##_Image|kage@8GgeI/btraetx3Jcy/p8Fxm2QHmCbSJfMKRtZvA0/img.png|alignCenter|data-origin-width="1430" data-origin-height="568" data-filename="스크린샷 2021-07-22 오전 12.42.18.png" width="641" height="255" data-ke-mobilestyle="widthOrigin"|reinforcement learning||_##]

<table style="border-collapse: collapse; width: 100%; height: 98px;" border="1" data-ke-align="alignLeft"><tbody><tr style="height: 20px;"><td style="width: 16.3953%; height: 20px;">agent</td><td style="width: 83.6047%; height: 20px;"><span style="color: #333333;">주어진 문제 상황에서 행동하는 <u>주체</u></span></td></tr><tr style="height: 20px;"><td style="width: 16.3953%; height: 20px;">action ($a_t$)</td><td style="width: 83.6047%; height: 20px;"><span style="color: #333333;">agent가 취하는 행동. ex) 게임에서 방향기 조정</span></td></tr><tr style="height: 20px;"><td style="width: 16.3953%; height: 20px;">state ($s_t$)</td><td style="width: 83.6047%; height: 20px;"><span style="color: #333333;">현재 시점에서 상태가 어떤지 나타내는 값의 집합</span></td></tr><tr style="height: 20px;"><td style="width: 16.3953%; height: 20px;">reward ($r_t$)</td><td style="width: 83.6047%; height: 20px;">agent가 취한 행동에 대한 보상</td></tr><tr style="height: 18px;"><td style="width: 16.3953%; height: 18px;">environment</td><td style="width: 83.6047%; height: 18px;">주어진 환경 (state, reward, policy.. 등등) <b>상태(state)</b><span style="color: #555555;"><span>&nbsp;</span>전환과<span>&nbsp;</span></span><b>보상(reward)</b><span style="color: #555555;">&nbsp;결정</span></td></tr></tbody></table>

강화학습이란, 어떤 Agent가 행동의 주체가 되어서 특정 timestamp t에서 Action $a\_t$를 취합니다.

$a\_t$를 취하면서 Environment가 바뀌고, State $s\_t$(현상태)는 $s\_{t+1}$로 바뀝니다. 그리고 행동에 따른 보상 $r\_t$가 주어집니다. 이런식으로 보상을 최대화 하는 쪽으로 반복하여 학습이 됩니다.

이때 보상이 충분하다고 판단이 되면 Environment는 State(현재상태)에 Terminal state(최종단계)임을 알리는 terminal state를 전달하며 학습이 끝납니다.

여태껏 알던 대부분의 학습과는 다르게 강화학습은 지도자가 없습니다.

Agent가 환경과의 상호작용을 통해 스스로 배우고 고쳐나가는 과정입니다. 현실세계에서의 예시를 들어보면 아래와 같습니다.

어려서 처음 두발 자전거를 배울때, 주변사람들에게서 배우는 경우도 있곘지만, 혼자 넘어지고 까지면서 배우는 아이도 있습니다. 이아이를 A(Agent)라고 하겠습니다.

A는 중심을 잃는 action을 취합니다. 행동에 대한 결과로 A는 넘어지면서 여기저기 까지고 피가 납니다. 이때 받는 보상은 부정적입니다. 따라서 A는 방금의 경험을 바탕으로 성공이라는 긍정적 보상을 향해 다시 도전합니다. 그리고 이 과정이 충분히 반복되면 A는 마침내 두발 자전거를 혼자 탈 수 있게 됩니다.

#### How can we mathematically formulate RL?

### By Markov Decision Process

MDP는 확률 모델의 일종으로 "**시간 t에서의 상태는 t−1에서의** **상태에만 영향을 받는다**"는 가정을 기반으로 고안되었습니다.

[##_Image|kage@bxxmah/btraiciOClp/TsjFr43AHxda2iRRp2TVb1/img.png|alignCenter|data-origin-width="1292" data-origin-height="344" data-filename="스크린샷 2021-07-22 오후 3.00.57.png" width="694" height="185" data-ke-mobilestyle="widthOrigin"|notation||_##][##_Image|kage@1PnSu/btradSlrufa/mUHdKoR4k7yAAiCK0ScKnk/img.png|alignCenter|data-origin-width="1416" data-origin-height="698" data-filename="스크린샷 2021-07-22 오후 3.03.15.png" width="734" height="362" data-ke-mobilestyle="widthOrigin"|||_##]

Time step t에서 state $s\_t$인 상황일때,

-   Agent는 Action $a\_t$를 취합니다.
-   reward $r\_t$와 state $s\_{t+1}$는 state와 action이 주어졌을때 reward 집합 R에서 샘플됩니다.
    -   reward와 state의 time step이 다른 이유는 $r\_t$는 $a\_t$에 대한 보상이고 $s\_{t+1}$은 $a\_t$이후의 State이기 때문입니다.

**Policy $\\pi$**

: 어떤 상태(state) A를 입력받아 취할 행동(action)을 output하는 함수로, 에이전트가 행동을 결정하기 위해 사용하는 알고리즘

강화학습의 목표 : 최적의 policy $\\pi \*$를 찾는것 = 누적된 보상액이 최대가 되게끔 하는 것

이해를 돕기 위해 간단한 MDP의 예제를 들어 설명하겠습니다.

[##_Image|kage@bBEaAj/btracOjwrlx/GXCWCuuLbR4bXA4LrjKq50/img.png|alignCenter|data-origin-width="1150" data-origin-height="524" data-filename="스크린샷 2021-07-22 오후 3.11.26.png" width="726" height="331" data-ke-mobilestyle="widthOrigin"|initial state||_##]

위와 같은 그리드가 있을때 Objective은 시작점에서 별모양이 있는곳까지 최소한의 거리로 가는것입니다. (시작점은 아무데나 주어질수 있습니다.)

강화학습의 목적은 보상을 "최대화" 하는것이기 때문에, 거리가 1 늘어날때마다 보상을 -1씩 주어서 결과적으로 거리를 최소화시키는 목적을 줍니다.

-   왼쪽 위에 actions의 집합 A를 보면 취할 수 있는 action은 상화좌우 총 4가지입니다.
-   action을 한번 취할때마자 reward와 state가 업데이트 됩니다.
-   위 예시에서 state란 어느 좌표에 위치해있는지를 의미합니다.

위에서 말했듯이, 강화학습의 궁극적 목적은 보상을 최대화 할 수 있는 optimal policy를 찾는것입니다. 

[##_Image|kage@b353tG/btradJuYA6E/tUoGNkuZD4OkkbJczur5mK/img.png|floatLeft|data-origin-width="380" data-origin-height="386" data-filename="스크린샷 2021-07-22 오후 3.17.32.png" width="208" height="211" data-ke-mobilestyle="widthOrigin"|||_##]

Random Policy를 적용하면 모든 state, 좌표에서 상하좌우로 갈 확률이 동등해집니다.

그래야만 random하다고 할 수 있겠죠. 따라서 운이 나쁘면 terminal state에 도달하지 못하고 빙글빙글 돌 수 도 있습니다. 

이건 좋은 경우가 아닙니다. optimal하지 못합니다! 

[##_Image|kage@bK4Esc/btrah97zdgU/Fhd4bLGRggpu3Zb5UGYFJ0/img.png|floatLeft|data-origin-width="358" data-origin-height="378" data-filename="blob" width="214" height="226" data-ke-mobilestyle="widthOrigin"|||_##]

Optimal Policy를 보면 보상을 최대화 할 수 있게 설계되었음을 알 수 있습니다.

A, B를 보면 상하좌우중 한 방향만 선택적으로 policy로 주고 있습니다.

A,B 에서 시작을 했든, 다른 칸에서 시작을 해 A or B에 도달을 했든 상관 없이,

일단 A, B에 도달했으면 = state가 A,B면, terminal state로 직행하는 것이 보상을 최대화하는 방법입니다. 

비슷한 원리로 각각의 state에서도 취했을때 보상이 유리할 경로로 action의 확률을 분배합니다.

Loss function 처럼 강화학습에서도 각각의 state와 action이 얼마나 좋은지를 정량적으로 매길 수 있는 함수들이 있습니다.

### 1\. Value Function

: initial state $s\_0$에 도달한 후 평균적으로 기대할 수 있는 sum of discounted reward

$$V^{\\pi}(s) = E\\left \[ \\sum\_{t\\geq 0}^{} \\gamma^tr\_t|s\_0=s,\\pi  \\right \]$$

### 2\. Q-Value Function

: initial state $s\_0$에서 action $a\_0$를 취했을때 평균적으로 기대할 수 있는 sum of discounted reward

$$Q^{\\pi}(s,a) = E\\left \[ \\sum\_{t\\geq 0}^{} \\gamma^tr\_t|s\_0=s,a\_o=a,\\pi  \\right \]$$

Q-value function에서 얻을 수 있는 최대 보상을 $Q^\*$이라 했을때, $Q^\*$은 아래의 **벨만 방적식**을 만족합니다.

-   벨만 방정식 = 현재 상태의 가치함수 Q\*(s,a)와 다음 상태의 가치함수Q\*(s'a') 사이의 관계

[##_Image|kage@M9t44/btrackJXWUU/N423FYdkLynkGkjRG01V31/img.png|alignCenter|data-origin-width="1394" data-origin-height="684" data-filename="스크린샷 2021-07-22 오후 4.46.00.png" width="728" height="357" data-ke-mobilestyle="widthOrigin"|||_##]

-   좌변에서 Expectation을 취하는 이유는 어떤 상태로 넘어갈지가 랜덤하기 때문에 그걸 평균내기 위함입니다.

벨만 방정식을 반복적으로 사용하면 optimal policy $\\pi \*$를 찾을 수 있게 됩니다.

[##_Image|kage@CeHt4/btranyzUZkI/iBNJVBZ7TEHAHPbedXDKTk/img.png|alignCenter|data-origin-width="1124" data-origin-height="354" data-filename="스크린샷 2021-07-23 오후 7.09.03.png" width="711" height="224" data-ke-mobilestyle="widthOrigin"|||_##]

하지만 이 경우에 가능 한 모든 Q(s,a)를 모든 (state, action) pair에 구해줘야 하기 때문에 계산량이 감당할 수 없을 정도가 됩니다.

앞에서 나왔던 Atari게임만 봐도, picel nxn으로 이루어진 state 공간에서,

모든가능한 경우의 조합을 구하는건 computationally 불가능합니다.

하지만 그렇다고 Q(s,a)를 못구할 리는 없죠.

Q(s,a)를 근사하는 function approximator를 사용하면 됩니다. 그리고 이렇게 복잡한 함수를 예측할때는 neural network를 사용하면 됩니다. !

### Q-Learning

최적의 action- value 함수 Q\*을 찾아서 optimal policy를 구하는것을 Q-Learning이라고 합니다.

여기에서 function estimator가 DL이면 Deep Q-Learning이 되는거고요.

[##_Image|kage@otmC8/btrao7O9sW2/tFUWU1NDuj9KaPooHx77ek/img.png|alignCenter|data-origin-width="1372" data-origin-height="560" data-filename="스크린샷 2021-07-23 오후 7.24.55.png" width="652" height="266" data-ke-mobilestyle="widthOrigin"|||_##]

Q-Network는 위와같은 forward/backward pass를 거쳐 학습이 됩니다. 하지만 문제점이 몇개 있습니다.

1.  학습데이터끼리 연결성(correlation)이 있거나 종속적이면 학습에 좋지 않은 영향을 끼칩니다. 근데 강화학습의 경우, 연속적이거나 인과관계가 있는 샘플들을 학습하는 경우가 있습니다. 따라서 **experience replay**를 사용해 보완을 해줘야 합니다.
2.  Q-function이 매우 복잡할 때 연산이 힘듭니다. 예를 들어 로봇이 물체를 쥐는 행동을 학습시킬때, 고려해야될 state는 모든 관절의 각도, 쥐는 힘, 이동 거리 등등 고려할게 너무 많습니다. 

문제들의 해결방법은 아래와 같습니다.

### Experience Replay

consecutive sample로 학습하는게 문제였기 때문에, 학습을 랜덤한 timestep의 mini-batch로 하는 방법이 Experience Replay입니다.

time step 별로 ($s\_t, a\_t, r\_t, s\_{t+1}$)를 **replay memory** 테이블에 저장하고 학습시 랜덤으로 배치추출하여 사용합니다.

### Policy Gradient

앞서 말했듯이, state space가 큰 복잡한 문제의 경우,

Q-function을 알아내어 모든 (state, action)조합의 value를 찾고 거기서부터 optimal policy를 찾는 과정은

computationally very challenging합니다. 그래서 나온 아이디어가 policy gradient입니다.

: Q-function을 찾는것을 건너뛰고, 그냥 바로 optimal policy를 찾자

[##_Image|kage@cCGPSz/btrakT6qCmj/9qRJhJtVYqxkhs8kbqFqUk/img.png|alignCenter|data-origin-width="1350" data-origin-height="372" data-filename="스크린샷 2021-07-24 오후 12.41.07.png" width="755" height="208" data-ke-mobilestyle="widthOrigin"|||_##]

우선 policy를 임의의 parameter를 사용해 초기화 합니다. 

$J$는 어떤 policy $\\pi\_{theta}$를 따랐을때 얻을 수 있는 미래 보상들의 평균입니다.

그리고 J를 사용해 아래의 식을 세울 수 있습니다.

$$\\theta ^\* = arg \\underset{\\theta}{max} J(\\theta)$$

이제 Gradient Ascent를 사용해 (arg max이기 때문에) parameter $\\theta$를 업데이트 합니다.

[##_Image|kage@ASmrB/btrao6wrJP9/46hkOHyijCk7pdeqZm5jY0/img.png|alignCenter|data-origin-width="1274" data-origin-height="636" data-filename="스크린샷 2021-07-24 오후 12.46.25.png" width="662" height="330" data-ke-mobilestyle="widthOrigin"|||_##]